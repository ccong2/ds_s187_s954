[
  {
    "objectID": "syllabus/index.html",
    "href": "syllabus/index.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Description\nUrban data science draws upon statistics, visualization, and computation to better understand and ultimately shape cities. This course emphasizes telling stories about cities using a set of descriptive approaches, quantitative and spatial analysis in R. We will learn how to describe community characteristics with small area census data, work with local administrative data, and think about how our analysis of quantitative data fit with other forms of data and engagement to fill in gaps in knowledge.\nLearning objectives:\n\nGain familiarity with R as a data analysis, mapping, and storytelling tool.\nBe able to identify relevant open datasets in support of your research question.\nArticulate the main arguments for your data analysis product and how it supports planning and decision-making regarding an issue of local/national significance.\n\n\n\nHow Will We Be Learning\nLecture: Class meetings are generally divided into lecture (Mondays) and laboratory sessions (Wednesdays) that focus on concepts and hand-on applications, respectively.\nLab: We will provide data science tutorials using R. Each lab tutorial aims to solve a specific urban data science problem in addition to building coding skills. Lab reports are due before the subsequent lab period and should be written independently.\nExtension readings: We provide a light material list that focuses on specific topics each week. These resources are meant to expand your knowledge and enhance your project completion capabilities.\nUrban data science project: The term project for the course will focus on integrating the tools of data science to explore a specific real-world planning issue or research question. This is a group project and students will define the scope of the project and identify specific deliverable(s) early in the semester. Reproducing an existing analysis or study using different data or an alternate study area is also acceptable for the term project.\n\n\nPrerequisites\nThis is a relatively fast-paced course so students can benefit from some prior knowledge working in R and RStudio. However, this is not a course prerequisite. Our first course sessions will focus on ensuring that we are all familiar with some of the basic work environment and methods which we’ll make use of over the semester.\n\n\nAssessment\n\n\n\n\n\n\n\nAssignment\nWeight\n\n\n\n\nLab Reports\nPackage Introduction\nProject Scope Memo\nProject Presentation\nAttendance\n40%\n10%\n10%\n30%\n10%\n\n\n\n\n\nKey Logistics\nLab Exercises: You will be working on lab on most Wednesday classes and submit a short report before the subsequent class. Details will be specified in each of the assignments distributed at the beginning of the lab session.\nPackage Introduction: Each student is expected to introduce one package to the class that is relevant to your research interests. The objectives are (1) to foster a collaborative approach to acquiring familiarity with the ever-increasing new packages, and (2) to identify valuable existing R packages applicable to your study. A separate guideline will be distributed at the beginning of the course.\nProject Scope Memo: Due Friday, Nov 17.\nThe purpose of this memo is to communicate the scope of your project (what you will do) and your strategies for collecting, visualizing, and analyzing data (how you will do it). A separate guideline will be distributed at the beginning of the course.\nProject Presentation: Week 8 (Dec 11 – Dec 13), class time.\n\n\nLate policy\nTo keep all students on a relatively level playing field, a late assignment will be accepted up until one week after the original due date for a loss of a half-letter grade (e.g., an A becomes an A-). After one week, late assignments will receive no credit and will not be accepted.\n\n\nSoftware\nWe use R and R Studio as the coding environment to develop analysis and applications. You can install the software on your personal computers from here and here.\n\n\nCommunication\nPlan on using our class Slack channel, email, and office hours to get help with troubleshooting problems as they arise in your work. I also encourage you to work with others in the class to troubleshoot problems - it is highly likely that others in the class have encountered similar problems, and this also allows us to create a repository of our problems and responses.\nEmail: I check emails quite frequently, but I will not always be able to respond to emails right away. Please plan accordingly so that you don’t miss deadlines.\nSlack: We have a Slack workspace that is accessible through Canvas for general communication, including homework Q&A, resource exchange, project collaboration, etc.\nOffice hours: Please consult the top of the syllabus for specific times. I will announce if there are any changes or exceptions. I’m happy to answer any specific coding questions, or chat and help shape the objective and scope of your projects.\n\n\nEthics\nAcademic Integrity: Violations to academic integrity are unacceptable at MIT and DUSP. Instances of misconduct include but are not limited to plagiarism, cheating, deliberately unauthorized use of course data and material.\nCollaboration Policy: While team collaboration is encouraged, students should specify their role and tasks in a project. A positive and constructive attitude for teamwork is essential for a successful completion of the course.\nDiversity and Inclusion: MIT highly values a diverse, friendly, respectful, and inclusive learning environment among students, faculty, and staff. We welcome all individuals regardless of their origin, citizenship, gender identity, sexual orientation, religious and pollical beliefs. Please contact me or departmental staff if you have any question / consideration regarding this."
  },
  {
    "objectID": "labs/lab1.html",
    "href": "labs/lab1.html",
    "title": "Get started with ",
    "section": "",
    "text": "This practice exercise provides some more structured exploration and practice with Quarto Document (R Markdown format). We will mix Markdown sections with code chunks, build familiarity with basic data types, and experiment with importing a tabular dataset. Because this is an in-class exercise, there is nothing you need to submit—the goal is to apply what we have read and seen in the lectures."
  },
  {
    "objectID": "labs/lab1.html#select-selects-a-subset-of-columns.",
    "href": "labs/lab1.html#select-selects-a-subset-of-columns.",
    "title": "Get started with ",
    "section": "Select: selects a subset of columns.",
    "text": "Select: selects a subset of columns.\nWhile in base R, we do:\ndataset[, c(\"Column1\", \"Column2\")]\nIn dplyr, we do:\ndataset |&gt; select(Column1, Column2)\nIn other words, we can simply insert the column names into the select function, without worrying about syntax like indexing, concatenation (c()), and the quotation marks.\nIn the energy dataset, we probably don’t need all of the 46 columns. So we can make it a smaller dataset by specifying a few columns to keep. Insert a new code chunk in your document like this one below. Here we are using the pipe |&gt; operator to “chain together” lines of code. You can type this symbol in using Shift+Ctrl/Command+M.\n\nenergy &lt;- energy |&gt;\n  select(\n    `Data Year`,\n    `BEUDO Category`,\n    Owner,\n    `Year Built`,\n    `Primary Property Type - Self Selected`,\n    `Total GHG Emissions (Metric Tons CO2e)`,\n    `Total GHG Emissions Intensity (kgCO2e/ft2)`,\n    Longitude,\n    Latitude\n  ) \n\nSome of the column names are surrounded by backticks (`), that’s because they include special characters or spaces, which deviate from standard naming conventions. The use of backticks is a means of preserving these unique naming attributes. Just keep typing the column names, dplyr will populate the correct names for you."
  },
  {
    "objectID": "labs/lab1.html#filter-select-a-subset-of-rows",
    "href": "labs/lab1.html#filter-select-a-subset-of-rows",
    "title": "Get started with ",
    "section": "filter: Select a subset of rows",
    "text": "filter: Select a subset of rows\nIn base R, we do this to pick observations by their values:\ndataset[dataset$place == “Boston\", ]\nIn dplyr, we do:\ndataset |&gt; filter(place == “Boston\")\nAgain, a simpler and more understandable syntax.\nNow let’s create a new dataset that only contains energy use records from MIT buildings and that are not missing the total GHG emission attribute. Take a look at how we achieve this using the following code, then proceed to insert a new code chunk in your document like the one below:\n\nmit_energy &lt;- energy |&gt; \n  filter(Owner == \"MASSACHUSETTS INSTITUTE OF TECHNOLOGY\") |&gt; \n  filter(!is.na(`Total GHG Emissions (Metric Tons CO2e)`))"
  },
  {
    "objectID": "labs/lab1.html#mutate-create-or-modify-columns",
    "href": "labs/lab1.html#mutate-create-or-modify-columns",
    "title": "Get started with ",
    "section": "mutate: create or modify columns",
    "text": "mutate: create or modify columns\nWe can change the values in a column either based on specified values or certain transformations. For example, below we are showing how to replace the full name of the institute with “MIT”.\n\nmit_energy |&gt; mutate(Owner = \"MIT\")\n\n# A tibble: 891 × 9\n   `Data Year` `BEUDO Category` Owner `Year Built` Primary Property Type - Sel…¹\n         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;                        \n 1        2015 Non-Residential  MIT           1994 College/University           \n 2        2017 Residential      MIT           1963 College/University           \n 3        2021 Non-Residential  MIT           2020 Office                       \n 4        2021 Non-Residential  MIT           1983 College/University           \n 5        2017 Non-Residential  MIT           1916 College/University           \n 6        2021 Non-Residential  MIT           1994 College/University           \n 7        2018 Non-Residential  MIT           1931 College/University           \n 8        2021 Non-Residential  MIT           1956 College/University           \n 9        2020 Residential      MIT           1999 Multifamily Housing          \n10        2016 Residential      MIT           1963 College/University           \n# ℹ 881 more rows\n# ℹ abbreviated name: ¹​`Primary Property Type - Self Selected`\n# ℹ 4 more variables: `Total GHG Emissions (Metric Tons CO2e)` &lt;dbl&gt;,\n#   `Total GHG Emissions Intensity (kgCO2e/ft2)` &lt;dbl&gt;, Longitude &lt;dbl&gt;,\n#   Latitude &lt;dbl&gt;\n\n\nYou can also use mutate to add new columns to your data frame that are calculated from existing columns. Here we are showing how to create a new column for building ages.\n\nmit_energy |&gt; mutate(Building_Age = 2023 - `Year Built`)\n\n# A tibble: 891 × 10\n   `Data Year` `BEUDO Category` Owner        `Year Built` Primary Property Typ…¹\n         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;               &lt;dbl&gt; &lt;chr&gt;                 \n 1        2015 Non-Residential  MASSACHUSET…         1994 College/University    \n 2        2017 Residential      MASSACHUSET…         1963 College/University    \n 3        2021 Non-Residential  MASSACHUSET…         2020 Office                \n 4        2021 Non-Residential  MASSACHUSET…         1983 College/University    \n 5        2017 Non-Residential  MASSACHUSET…         1916 College/University    \n 6        2021 Non-Residential  MASSACHUSET…         1994 College/University    \n 7        2018 Non-Residential  MASSACHUSET…         1931 College/University    \n 8        2021 Non-Residential  MASSACHUSET…         1956 College/University    \n 9        2020 Residential      MASSACHUSET…         1999 Multifamily Housing   \n10        2016 Residential      MASSACHUSET…         1963 College/University    \n# ℹ 881 more rows\n# ℹ abbreviated name: ¹​`Primary Property Type - Self Selected`\n# ℹ 5 more variables: `Total GHG Emissions (Metric Tons CO2e)` &lt;dbl&gt;,\n#   `Total GHG Emissions Intensity (kgCO2e/ft2)` &lt;dbl&gt;, Longitude &lt;dbl&gt;,\n#   Latitude &lt;dbl&gt;, Building_Age &lt;dbl&gt;\n\n\nThere is no &lt;- operator in these two code chunks. We are observing the results but the resulting tables are not saved in an object."
  },
  {
    "objectID": "labs/lab1.html#group_by-summarise",
    "href": "labs/lab1.html#group_by-summarise",
    "title": "Get started with ",
    "section": "Group_by + Summarise",
    "text": "Group_by + Summarise\nSummarise is usually used in conjunction with group_by because the latter changes the scope from operating on the entire dataset to operating on it group-by-group. Go ahead and run the following code:\n\nmit_energy |&gt; \n  group_by(`Data Year`) |&gt; \n  summarise(count = n())\n\n# A tibble: 7 × 2\n  `Data Year` count\n        &lt;dbl&gt; &lt;int&gt;\n1        2015   129\n2        2016   134\n3        2017   130\n4        2018   134\n5        2019   111\n6        2020   115\n7        2021   138\n\n\nWe use group_by such that observations (i.e., rows) are grouped according to Data Year, which is the year when the energy record was taken. The result is then passed to summarise to generate a total number of records per year. By default, the n() function creates a new attribute (i.e., column), which we here name as “count”.  \nBelow we are using the same group_by + summarise chain to calculate the average GHG emissions of all buildings, and the average GHG emission intensity (use the column Total GHG Emissions Intensity (kgCO2e/ft2)). Note that we are now giving new names to each of the new columns.\n\nmit_energy |&gt; \n  group_by(year = `Data Year`) |&gt; \n  summarise(count = n(),\n            avg_emission = mean(`Total GHG Emissions (Metric Tons CO2e)`),\n            avg_intensity = mean(`Total GHG Emissions Intensity (kgCO2e/ft2)`))\n\n# A tibble: 7 × 4\n   year count avg_emission avg_intensity\n  &lt;dbl&gt; &lt;int&gt;        &lt;dbl&gt;         &lt;dbl&gt;\n1  2015   129        1585.          13.4\n2  2016   134        1445.          13.3\n3  2017   130        1523.          13.5\n4  2018   134        1472.          13.2\n5  2019   111        1522.          12.8\n6  2020   115        1396.          11.0\n7  2021   138        1415.          11.5\n\n\n\nYour practice\nInsert a few new code chunks below this one to document your code and show your results. \n\nFrom the mit_energy dataset, create a subset of all non-residential buildings, which were built before the year 2000. (Hint: which function would you use?). How many such buildings are there?\nFrom the mit_energy dataset, compare the GHG emissions by property type (Hint: which column has that information?), and generate a table that shows the following results:\n\n\nYou can create this table mostly by modifying the last code chunk (labeled “annual_mean”), however, there are a few small things you can experiment on:\n\nThe calculated average numbers in this table are rounded to 2 decimals, how to achieve that?\nThe table is arranged in descending order based on the “avg_emission” column, how to do that? (Hint)\n\nWe are already trying to ask questions and find hints of interesting stories from the dataset, which is what Exploratory Data Analysis (EDA) is all about. If the results so far look interesting/surprising/expected to you, write a few sentences describing what you see from the analysis.\n\nLastly, we will insert a map to complete your working document! This dataset includes “Longitude” and “Latitude” columns, which I like because it indicates that location information is available and can be visualized.\nAdd the following code to your document, and you should be able to run it and see a map. (If your R says it can’t find mapview, run the line install.packages(\"mapview\"))\n\n\n#install.packages(\"mapview\")\nlibrary(mapview)\nmapview(\n  mit_energy,\n  xcol = \"Longitude\", ycol = \"Latitude\",\n  crs = 4326,\n  grid = FALSE\n)\n\n\n\n\n\n\n\nNow Save, Render your document again. You have now created a pretty, multi-media document using R!\n------\nIn this lab we have introduced how to create and develop a Quarto Document. We have also introduced some commonly-used dplyr funtions including select, filter, mutate, group_by and summarise. This is the beginning of our data wrangling and leads to the work in Week 2."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Urban science draws on statistics, visualization, and spatial analysis techniques to gain deeper insights into cities and actively contribute to their development. In this course, we’ll dive into the dynamic world of urban science by learning how to tell stories about cities and neighborhoods, covering a range of topics including demographic analysis, health and transportation, and using R as our primary quantitative analysis and interactive visualization tool.\n\n\n\n\n\n\n\nCourse Information\n\nClass Time: M, W: 9:30-11:00 AM\nLocation: Building 9-450\nCanvas Site: https://canvas.mit.edu/courses/23126"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "howto/setupr.html",
    "href": "howto/setupr.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Urban analytics draws upon statistics, visualization, and computation to better understand and ultimately shape cities. This course emphasizes telling stories about cities and neighborhoods covering a set of fundamental concepts of descriptive approaches, quantitative and spatial analysis in R, and principles of reproducible data analysis. Students learn to communicate the results of visualization and analysis for use in decision-making and policy development and to critique those processes."
  },
  {
    "objectID": "howto/setupr.html#this-website-is-under-construction-please-check-back-later-for-updates",
    "href": "howto/setupr.html#this-website-is-under-construction-please-check-back-later-for-updates",
    "title": "Applied Data Science for Cities",
    "section": "This website is under construction, please check back later for updates! 😄",
    "text": "This website is under construction, please check back later for updates! 😄"
  },
  {
    "objectID": "labs/index.html",
    "href": "labs/index.html",
    "title": "Schedule Overview",
    "section": "",
    "text": "Schedule\nW01 (Oct 23 - Oct 27):\n\nCourse Overview\nLab1: Cambridge Building Energy: R, Quarto, dplyr essentials\n\nW02 (Oct 30 - Nov 3):\n\nExploratory Data Analysis\nLab 2: Opportunity Zones: tidyverse, ggplot2 packages\n\nW03 (Nov 6 - Nov 10):\n\nCensus Data and Demographic Analysis\nLab 3: Racial Distribution: tidycensus, tidyr packages\n\nW04 (Nov 13 – Nov 17):\n\nCreate interactive graphs and maps\nLab 4: Airbnb in Chicago: plotly and leaflet packages\n\nW05 (Nov 20 – Nov 22):\n\nSpatial Analysis; Obtain data from multiple sources\nLab 5: Obesogenic Built Environment: sf, osmdata packages\n\nW06 (Nov 27 – Dec 1):\n\nWeb Storytelling I\nShiny apps and examples \n\nW07 (Dec 4 – Dec 8):\n\nWeb Storytelling II\nWork on projects\n\nW08 (Dec 11 – Dec 13):\n\nPresentation"
  },
  {
    "objectID": "labs/lab2.html",
    "href": "labs/lab2.html",
    "title": "Exploratory Data Analysis with ",
    "section": "",
    "text": "This week’s Lab Exercise focuses on the dplyr package and the ggplot2 package. It also begins to engage with data visualization best practices by demonstrating how to create and interpret a variety of graphics.\nExploratory data analysis (EDA) is a phase of a larger data science workflow that emphasizes getting to know the data before rushing to analyze it. EDA typically involves the creation and interpretation of graphics in order to build familiarity and gain fundamental insights that can inform more sophisticated analyses later on. There are several overarching goals of exploratory data analysis, including:\n\nTo determine if there are any problems with your dataset.\nTo determine whether the question you are asking can be answered by the data that you have.\nTo begin formulating an answer to your question."
  },
  {
    "objectID": "labs/lab2.html#download-data-and-load-packages",
    "href": "labs/lab2.html#download-data-and-load-packages",
    "title": "Exploratory Data Analysis with ",
    "section": "Download data and load packages",
    "text": "Download data and load packages\nNow please navigate to Urban Institute’s website about Opportunity Zones, find the link “Download tract-level data on all Opportunity Zones”, and download this dataset to your “data” folder within your Lab 2 project folder.\nStart a new .qmd (or .Rmd) file and remove the template texts.\nTo stay organized, we should load packages at the beginning of our markdown document. These are the three packages we are going to use today.\n\nlibrary(readxl)\nlibrary(tidyverse)\nlibrary(DataExplorer)"
  },
  {
    "objectID": "labs/lab2.html#initial-data-cleaning",
    "href": "labs/lab2.html#initial-data-cleaning",
    "title": "Exploratory Data Analysis with ",
    "section": "Initial data cleaning",
    "text": "Initial data cleaning\nThe Urban Institute has coded the designated variable as either taking a value of 1 when designated, or NA when not. We can recode the NA values in DesignatedOZ for legibility. In the following code, we use the dplyr function: mutate to modify DesignatedOZ in place. We replaced the numbers with texts since the NA and 1 here have no mathematical meaning.\n\nozs &lt;- ozs |&gt; mutate(DesignatedOZ = \n                ifelse(is.na(DesignatedOZ), \"Not Designated\", \"Designated\"))\n\nThis modification will make it much easier for us to make a quick count of both types of areas.\n\nozs |&gt; \n  count(DesignatedOZ) \n\n# A tibble: 2 × 2\n  DesignatedOZ       n\n  &lt;chr&gt;          &lt;int&gt;\n1 Designated      8764\n2 Not Designated 33414\n\n\nOr, a cross-tabulation of both types of areas by state.\n\nozs |&gt; \n  count(state, DesignatedOZ)\n\n# A tibble: 108 × 3\n   state          DesignatedOZ       n\n   &lt;chr&gt;          &lt;chr&gt;          &lt;int&gt;\n 1 Alabama        Designated       158\n 2 Alabama        Not Designated   677\n 3 Alaska         Designated        25\n 4 Alaska         Not Designated    43\n 5 American Samoa Designated        16\n 6 Arizona        Designated       168\n 7 Arizona        Not Designated   702\n 8 Arkansas       Designated        85\n 9 Arkansas       Not Designated   435\n10 California     Designated       879\n# ℹ 98 more rows\n\n\nThere are a few columns (such as SE_Flag) that we won’t need for this analysis. We can use select in dplyr function to select a subset of columns to work on. select allows you to retain specified columns. If there is a minus sign in front, that means to drop these specified columns.\n\nozs &lt;- ozs |&gt; select(-c(dec_score, SE_Flag, pctown, Metro, Micro, NoCBSAType))\n\nOne of the characteristics tracked in the Urban Institute data is the median household income for each tract. We might question whether there’s a difference in the median household income for designated and not-designated census tracts. Let’s see if we can calculate the mean of the median household income values:\n\nmean(ozs$medhhincome)\n\n[1] NA\n\n\nIt returns NA. But it doesn’t indicate an issue with our code. The reason for this is the presence of missing values in this column. When dealing with numerical variables, NA values affect calculation results because we can’t decide if the missing values are larger or smaller than the others and we don’t know how to calculate them.\nTake another look at the ozs data. Sort the medhhincome column by clicking the little triangles appearing on the column name. Drag down to the bottom of the dataset, then you will see quite a few of NAs in the Census demographic columns.\nHow many missing values are there, and how many would be a hurdle for my analysis? It will be great to have a sense of completeness in terms of what proportion of a field actually holds data. DataExplorer is a handy tool that offers functions to quickly understand the completeness of datasets.\n\nDataExplorer::plot_missing(ozs)\n\n\n\n\nplot_missing calculates the proportion of missing values in a given variable, and makes some judgemental calls of whether the missing is significant, indicated by “Good”, “OK”, and “Bad”. (Check out the documentation by typing ?plot_missing in your console. What are the default ranges for these three categories?) Overall, most of our columns have a very small percentage of missing values (less than 1%) and would not create significant representative issues. However, when performing calculations, we need to include the na.rm = TRUE argument, indicating that we are calculating based on the available 99%.\n\nmean(ozs$medhhincome, na.rm = TRUE)\n\n[1] 42152.76\n\n\nNote: R can do the same thing in different ways. This is especially true as you get further and further into the weeds - some niche packages are able to perform general tasks, in more or less verbose ways. For example, in dplyr language, you may do the same calculation like this:\n\nozs |&gt; summarise(meanvalue = mean(medhhincome, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  meanvalue\n      &lt;dbl&gt;\n1    42153."
  },
  {
    "objectID": "labs/lab2.html#exercise-1",
    "href": "labs/lab2.html#exercise-1",
    "title": "Exploratory Data Analysis with ",
    "section": "Exercise 1",
    "text": "Exercise 1\nWhile we are now examining the nationwide dataset, you are going to conduct some focused analysis for Massachusetts. Please insert a new code chunk below to create a new object ozs_ma that extracts all tracts in Massachusetts. Then add some more code chunks and texts to “catch” your responses to the following questions.\n\nWhich of the variables (columns) are continuous and which are categorical (e.g., character)?\n\nRecall that a variable is categorical if it can only take one of a small set of values and continuous if it can take any of an infinite set of ordered values.\nWhich function or approach did you use to answer this question?\n\nIn Massachusetts, how many eligible census tracts are designated as Opportunity Zones, and how many are not designated?\n\nWhich function or approach did you use to answer this question?\n\nChoose one of the following variables: medhhincome, vacancyrate, unemprate, pctwhite, pctblack, pctHispanic, pctover64, HSorlower . What are the average values of your variable for both designated and not designated tracts in Massachusetts?\n\nWhich function or approach did you use to answer this question?"
  },
  {
    "objectID": "labs/lab2.html#boxplot",
    "href": "labs/lab2.html#boxplot",
    "title": "Exploratory Data Analysis with ",
    "section": "Boxplot",
    "text": "Boxplot\nNow we are ready to create some visual representations of our data. The code below creates a boxplot to contrast the distribution of poverty rates between designated opportunity zones and undesignated zones. We are using what should now be familiar conventions to construct the graphic beginning with the ggplot function, then adding more features with the + operator and other functions listed in the package reference.\n\nggplot(data = ozs): This is the main plotting function. ozs is your dataset we use.\ngeom_boxplot(): Recall that geometric layers are called geoms. It tells R what kind of geometry you want to use visualize the data.\naes(x = DesignatedOZ, y = PovertyRate): The aes() function is where you tell ggplot which variable goes on the x axis followed by which variable goes on the y axis.\nThe third aesthetic element is fill, which indicates the filled color of the boxplot. Accordingly, we use the fill argument in the labs function to set the name of the legend.\nWe used a new function scale_y_continuous to specify y axis properties. Here we are making sure the poverty rate are labeled as percentages. If you remove this line, they will by default show as decimal numbers.\n\n\nggplot(data = ozs) +\n  geom_boxplot(aes(x = DesignatedOZ, y = PovertyRate, fill = DesignatedOZ), na.rm = TRUE) + \n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Opportunity Zone Eligible Tracts\", y = \"Poverty Rate\", fill = \"Tracts\")\n\n\n\n\nBy comparing the 50th percentile (the horizontal line inside each box) we can see that tracts designated as Opportunity Zones have a higher poverty rate compared with those not designated. The heights of the boxes themselves give us an indication of how closely around the median all values in the dataset are concentrated—the degree of dispersion or spread. The vertical lines are called whiskers and extend upward and downward to the lowest values that are not candidates for outlier status. An outlier is an unusual value that could potentially influence the results of an analysis. These are indicated with dots in the boxplot.\nOutliers can be legitimate values, and require the exercise of judgment on the part of the analyst and may be removed or excluded from the analysis or imputed. There are a variety of criteria that have been offered, but you will address this question from a more statistical perspective in your other courses that introduce linear methods."
  },
  {
    "objectID": "labs/lab2.html#density-plot",
    "href": "labs/lab2.html#density-plot",
    "title": "Exploratory Data Analysis with ",
    "section": "Density plot",
    "text": "Density plot\nBy modifying the last code chunk, we can make a density plot to describe the distribution of poverty rate. A density plot can be understood as a smoothed version of the histogram, and provides a more direct view of of the shape and peaks in the data. The x-axis typically represents the range of values for the variable of interest, while the y-axis represents the probability density (how likely it is for the variable to take on a particular value within that range).\n\nIn the code below, we didn’t provide a variable that what goes to the y-axis. Where does the value “density” come from? Many graphs, like boxplot, plot the raw values of your dataset. But other graphs, like histograms and density plots, calculate new values to plot. Here a density takes the count of data points at discrete poverty rate levels and smooths it out into a continuous curve. Then calculated values (probability density) go to the y-axis.\n\n\nggplot(data = ozs) +\n  geom_density(aes(x = PovertyRate, fill = DesignatedOZ), na.rm = TRUE) + \n  scale_x_continuous(labels = scales::percent) +\n  labs(x = \"Poverty Rate\", fill = \"Tracts\")"
  },
  {
    "objectID": "labs/lab2.html#combinations-of-basic-graphs-to-create-composite-views",
    "href": "labs/lab2.html#combinations-of-basic-graphs-to-create-composite-views",
    "title": "Exploratory Data Analysis with ",
    "section": "Combinations of basic graphs to create composite views",
    "text": "Combinations of basic graphs to create composite views\nOne of the coolest thing about ggplot is that we can plot multiple geom_ on top of each other. For instance, we can combine the two plots above, to show both visually appealing curves and essential statistics (medians, quartiles, outliers, etc.) The following code uses two geom_(Check out geom_violin for more!), and introduces several new and helpful arguments for fine-tuning the cosmetics.\n\ntrim = FALSE: If TRUE (default), trim the tails of the violins to the range of the data. If FALSE, don’t trim the tails and show the complete distribution.\nalpha = 0.5: the transparency of the plotting area.\ncoord_flip(): whether the y axis is displayed horizonally or vertically.\nlegend.position = \"none\": the position of legend (“left”, “right”, “bottom”, “top”, or two-element numeric vector), or not showing the legend (“none”).\n\n\nggplot(ozs) +\n  geom_violin(aes(x = DesignatedOZ, y = PovertyRate, fill = DesignatedOZ), trim = FALSE, alpha = 0.5) +\n  geom_boxplot(aes(x = DesignatedOZ, y = PovertyRate), colour = \"black\", width = .15, alpha = 0.8) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(\n    x = \"Opportunity Zone Eligible Tracts\",\n    y = \"Poverty Rate\",\n    title = \"Distribution of Poverty Rate\"\n  ) +\n  coord_flip() +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "labs/lab2.html#exercise-2",
    "href": "labs/lab2.html#exercise-2",
    "title": "Exploratory Data Analysis with ",
    "section": "Exercise 2",
    "text": "Exercise 2\nFocus on your selected variable in Exercise 1 and your data specific to Massachusetts. Explore the distribution of your variable of interest.\n\nCreate one graphical representation that contrasts the results in designated tracts and in undesignated tracts.\nInterpret the graphic you have created and include 2-3 sentences of text explanation, for example:\n\nIs it a more flat/homogeneous distribution, or a more skewed distribution in terms of fewer observations belonging to the higher-value groups? Does that align with your expectation for this variable?\nWhat can we say about the difference in demographic/economic conditions between designated and not designated census tracts in Massachusetts?\nFeel free to consult the R Graph Gallery and Aesthetic specifications for additional resources."
  },
  {
    "objectID": "labs/lab2.html#scatterplot",
    "href": "labs/lab2.html#scatterplot",
    "title": "Exploratory Data Analysis with ",
    "section": "Scatterplot",
    "text": "Scatterplot\nWe are often interested in bivariate relationships or how two variables relate to one another. Scatterplots are often used to visualize the association between two continuous variables. They can reveal much about the nature of the relationship between two variables.\nLet’s use your subset of Massachusetts data to perform this part of analysis. We can definitely use the entire dataset, but there will be over 40,000 points showing on the graph, which will not be pleasing to the eye.\n\nozs_ma &lt;- ozs |&gt; filter(state == \"Massachusetts\") \n\nWe begin by creating a scatterplot of poverty rate and unemployment rate. Note that we used theme_bw, which is a theme template for a cleaner look.\n\nggplot(ozs_ma) +\n  geom_point(aes(x = unemprate, y = PovertyRate)) +\n  labs(x = \"Unemployment rate\",\n       y = \"Poverty rate\",\n       title = \"Poverty rate vs. unemployment rate in Opportunity Zone eligible tracts\", \n       subtitle = \"State of Massachusetts\",\n       caption = \"Source: Urban Institute (2018)\") + \n  theme_bw()\n\n\n\n\nIt is generally easy to recognize patterns in a graphical display. As we move from left to right along the x-axis (i.e., as the unemployment rate creases), the amount of poverty rate reported also increases.\nAs a complement to a scatterplot, we can use the stats::cor function to calculate the (Pearson by default, see ?cor for other options) correlation between any continuous variables in the dataset. The DataExplorer package is also designed to help us quickly understand patterns in our data. We demonstrate both in the following code.\nIf you are unfamiliar with reading a correlation matrix, the values range between -1 and 1 where:\n\n-1 indicates a perfectly negative linear correlation between two variables\n0 indicates no linear correlation between two variables\n1 indicates a perfectly positive linear correlation between two variables\n\n\nozs_ma |&gt; select(Population:medrent, pctwhite:BAorhigher) |&gt; \n  stats::cor(use = \"complete.obs\")\n\n                Population medhhincome PovertyRate  unemprate    medvalue\nPopulation    1.0000000000   0.1971310 -0.17219731 -0.1255690  0.05060663\nmedhhincome   0.1971310072   1.0000000 -0.74280330 -0.5823090  0.45672297\nPovertyRate  -0.1721973144  -0.7428033  1.00000000  0.5848120 -0.15673933\nunemprate    -0.1255690086  -0.5823090  0.58481204  1.0000000 -0.32627434\nmedvalue      0.0506066324   0.4567230 -0.15673933 -0.3262743  1.00000000\nmedrent       0.1434824928   0.6494977 -0.32058250 -0.4139894  0.60946764\npctwhite      0.0007742062   0.4334837 -0.58166274 -0.4583927  0.01479243\npctBlack      0.0305972149  -0.2081070  0.24067220  0.3002354  0.07662521\npctHispanic  -0.0610330071  -0.4470198  0.53817440  0.4107687 -0.24108925\npctAAPIalone  0.1179507879   0.1360442  0.05777953 -0.1516819  0.38403023\npctunder18    0.0075944281  -0.3886837  0.28931873  0.4263952 -0.48597184\npctover64    -0.0200934534   0.1116477 -0.40837729 -0.2236285 -0.08790793\nHSorlower    -0.0510755304  -0.6192728  0.38428550  0.5080771 -0.59138685\nBAorhigher    0.0317046620   0.5837037 -0.24483958 -0.4850706  0.71630822\n                 medrent      pctwhite    pctBlack pctHispanic pctAAPIalone\nPopulation    0.14348249  0.0007742062  0.03059721 -0.06103301   0.11795079\nmedhhincome   0.64949767  0.4334836763 -0.20810703 -0.44701977   0.13604425\nPovertyRate  -0.32058250 -0.5816627414  0.24067220  0.53817440   0.05777953\nunemprate    -0.41398938 -0.4583927470  0.30023542  0.41076874  -0.15168191\nmedvalue      0.60946764  0.0147924343  0.07662521 -0.24108925   0.38403023\nmedrent       1.00000000  0.0592656294 -0.01942961 -0.21839927   0.37309846\npctwhite      0.05926563  1.0000000000 -0.64391155 -0.72603098  -0.14608318\npctBlack     -0.01942961 -0.6439115534  1.00000000  0.05756055  -0.07150764\npctHispanic  -0.21839927 -0.7260309801  0.05756055  1.00000000  -0.16352352\npctAAPIalone  0.37309846 -0.1460831806 -0.07150764 -0.16352352   1.00000000\npctunder18   -0.47107334 -0.4866220916  0.28987714  0.52756318  -0.29072583\npctover64    -0.20439363  0.5440272257 -0.22702697 -0.42430798  -0.23991183\nHSorlower    -0.59290214 -0.4609030805  0.16871183  0.56358387  -0.25180077\nBAorhigher    0.67357448  0.3278345358 -0.19790460 -0.42390213   0.38422347\n               pctunder18    pctover64   HSorlower   BAorhigher\nPopulation    0.007594428 -0.020093453 -0.05107553  0.031704662\nmedhhincome  -0.388683691  0.111647722 -0.61927277  0.583703714\nPovertyRate   0.289318732 -0.408377288  0.38428550 -0.244839584\nunemprate     0.426395199 -0.223628488  0.50807715 -0.485070585\nmedvalue     -0.485971837 -0.087907931 -0.59138685  0.716308223\nmedrent      -0.471073339 -0.204393629 -0.59290214  0.673574479\npctwhite     -0.486622092  0.544027226 -0.46090308  0.327834536\npctBlack      0.289877141 -0.227026967  0.16871183 -0.197904602\npctHispanic   0.527563183 -0.424307981  0.56358387 -0.423902133\npctAAPIalone -0.290725828 -0.239911831 -0.25180077  0.384223470\npctunder18    1.000000000 -0.248306018  0.68045309 -0.718007353\npctover64    -0.248306018  1.000000000 -0.14789121 -0.003268322\nHSorlower     0.680453091 -0.147891209  1.00000000 -0.923184800\nBAorhigher   -0.718007353 -0.003268322 -0.92318480  1.000000000\n\n\n\nozs_ma |&gt; select(Population:medrent, pctwhite:BAorhigher) |&gt; \n  na.omit() |&gt; \n  DataExplorer::plot_correlation()\n\n\n\n\nAn additional note to the code above is that we selected several continuous variables that we want to inspect, and removed NA values (use = \"complete.obs\") so that the correlation values can be correctly calculated.\nWhat can we do if we are interested in statistical associations between categorical variables? The typical approach is to use a chi-squared test (see the chisq.test function in the stats package) in conjunction with visual tools like barcharts. We won’t delve deeply into the statistical aspect, but we will learn the useful bar plots for displaying summary statistics of categorical data."
  },
  {
    "objectID": "labs/lab2.html#bar-plot",
    "href": "labs/lab2.html#bar-plot",
    "title": "Exploratory Data Analysis with ",
    "section": "Bar plot",
    "text": "Bar plot\nIn the following code, you can see our familiar group_by + summarise process used to calculate the average median house income by county in Massachusetts. This summarized table is then piped to ggplot() for visualization.\nIf you are unsure about how the output was generated, you can highlight and run part of the code and observate the intermediate results. For example, you can run\n\nozs_ma |&gt; group_by(county, DesignatedOZ) |&gt;summarise( Tracts = n(), Income = mean(medhhincome, na.rm=TRUE)),\n\nsee what it gives you, then add one more line at a time to observe the changes accordingly.\n\nozs_ma |&gt; \n  group_by(county, DesignatedOZ) |&gt;  \n  summarise(\n    Tracts = n(),\n    Income = mean(medhhincome, na.rm=TRUE)) |&gt; \n  ggplot() +\n  geom_col(aes(x = county, y = Income, fill = DesignatedOZ))"
  },
  {
    "objectID": "labs/lab2.html#exercise-3",
    "href": "labs/lab2.html#exercise-3",
    "title": "Exploratory Data Analysis with ",
    "section": "Exercise 3",
    "text": "Exercise 3\nTake a few minutes to read this bar chart below:\n\nThere are a few obvious changes in this graph compared with what we just created before. Overall, it looks nicer…How can we modify our code above to replicate the bar chart in this image? You’ll notice that you can achieve most of the features by tweaking our previous examples, plus a little bit more exploration. In a new code chunk, please copy and paste our last bar chart code, and try your best to address the following questions.\n\nThe stacking is performed automatically by the position= argument in geom_col(). More explainations here. If you don’t want a stacked bar chart, you can use one of the three other options: “identity”, “dodge”, or “fill”.\nThe x-axis labels are titled to 45 degrees. How can I achieve this? Hint.\nThe labels on the y-axis are formatted in thousands with commas. This can be achieved by modifying the function scale_y_continuous(labels = scales::percent) we have seen before. Hint.\nLastly, the counties are not arranged alphabetically, but rather by the income values mapped to the y-axis, starting from large to small. How can I achieve this? Hint.\nPlease add the title, subtitle, x- and y-axis labels, and the data source annotation to your bar chart.\nThe background looks much cleaner. Please choose a theme template for your bar chart."
  },
  {
    "objectID": "labs/lab2.html#optional-scatterplot-with-marginal-histograms",
    "href": "labs/lab2.html#optional-scatterplot-with-marginal-histograms",
    "title": "Exploratory Data Analysis with ",
    "section": "Optional: Scatterplot with marginal histograms",
    "text": "Optional: Scatterplot with marginal histograms\nThis requires a new package ggExtra. But the other syntax should be familiar now.\n\n#install.packages(\"ggExtra\")\np &lt;- ggplot(ozs_ma) + \n  geom_point(aes(x = pctBlack, y = PovertyRate, color = DesignatedOZ)) + \n  theme_bw()\nggExtra::ggMarginal(p, type = \"histogram\", groupFill = TRUE)"
  }
]