[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "howto/index.html",
    "href": "howto/index.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Urban analytics draws upon statistics, visualization, and computation to better understand and ultimately shape cities. This course emphasizes telling stories about cities and neighborhoods covering a set of fundamental concepts of descriptive approaches, quantitative and spatial analysis in R, and principles of reproducible data analysis. Students learn to communicate the results of visualization and analysis for use in decision-making and policy development and to critique those processes."
  },
  {
    "objectID": "howto/index.html#this-website-is-under-construction-please-check-back-later-for-updates",
    "href": "howto/index.html#this-website-is-under-construction-please-check-back-later-for-updates",
    "title": "Applied Data Science for Cities",
    "section": "This website is under construction, please check back later for updates! üòÑ",
    "text": "This website is under construction, please check back later for updates! üòÑ"
  },
  {
    "objectID": "howto/setupr.html",
    "href": "howto/setupr.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Urban analytics draws upon statistics, visualization, and computation to better understand and ultimately shape cities. This course emphasizes telling stories about cities and neighborhoods covering a set of fundamental concepts of descriptive approaches, quantitative and spatial analysis in R, and principles of reproducible data analysis. Students learn to communicate the results of visualization and analysis for use in decision-making and policy development and to critique those processes."
  },
  {
    "objectID": "howto/setupr.html#this-website-is-under-construction-please-check-back-later-for-updates",
    "href": "howto/setupr.html#this-website-is-under-construction-please-check-back-later-for-updates",
    "title": "Applied Data Science for Cities",
    "section": "This website is under construction, please check back later for updates! üòÑ",
    "text": "This website is under construction, please check back later for updates! üòÑ"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Urban science draws on statistics, visualization, and spatial analysis techniques to gain deeper insights into cities and actively contribute to their development. In this course, we‚Äôll dive into the dynamic world of urban science by learning how to tell stories about cities and neighborhoods, covering a range of topics including demographic analysis, health and transportation, and using R as our primary quantitative analysis and interactive visualization tool.\n\n\n\n\n\n\n\nCourse Information\n\nClass Time: M, W: 9:30-11:00 AM\nLocation: Building 9-450\nCanvas Site: https://canvas.mit.edu/courses/23126"
  },
  {
    "objectID": "labs/index.html",
    "href": "labs/index.html",
    "title": "Schedule Overview",
    "section": "",
    "text": "Schedule\nW01 (Oct 23 - Oct 27): What Does Urban Data Science Do\nW02 (Oct 30 - Nov 3): Exploratory Data Analysis\nW03 (Nov 6 - Nov 10): Census Data and Demographic Analysis\nW04 (Nov 13 ‚Äì Nov 17): Create interactive graphs and maps\nW05 (Nov 20 ‚Äì Nov 22): Spatial Analysis\nW06 (Nov 27 ‚Äì Dec 1): Web Storytelling I\nW07 (Dec 4 ‚Äì Dec 8): Web Storytelling II\nW08 (Dec 11 ‚Äì Dec 13): Presentation"
  },
  {
    "objectID": "labs/lab1.html",
    "href": "labs/lab1.html",
    "title": "Get started with ",
    "section": "",
    "text": "This practice exercise provides some more structured exploration and practice with Quarto Document (R Markdown format). We will mix Markdown sections with code chunks, build familiarity with basic data types, and experiment with importing a tabular dataset. Because this is an in-class exercise, there is nothing you need to submit‚Äîthe goal is to apply what we have read and seen in the lectures.\n\nPractice formatting text with Quarto\nAfter you download today‚Äôs lab folder, launch RStudio by double clicking the ‚Äúlab1.Rproj‚Äù file. Go to File - New File - Quarto Document to create a new Quarto document. The prompt shown below will appear. Type in a document title and your name.\n\nYou will see a template file. At the very top of the notebook we have the YAML or ‚ÄúYet Another Markdown Language‚Äù header which begins and ends with three dashes ---. At the very least, it specifies the title, author and output format when the document is rendered. There can be other options specified in the YAML, particularly if you are rendering to a format other than HTML (see the reading from this week).\nLet‚Äôs add another line to the YAML header:\n\ndate: &lt;insert the date that the file is created&gt;.\n\nBelow the YAML is where the Markdown sections typically begin. The interface looks pretty much like a Word document. There is a toolbar that allows you to make bold or italic text, create a bullet list, insert a link or an image, insert a code block, etc.\nOn the very left of this toolbar, click the ‚ÄúSource‚Äù button to switch Markdown editing mode. These are sections of text that usually explain or contextualize the code and graphics and are formatted using Markdown syntax. For example:\n\n#: a header element.\n**: bold text.\n*: italic text.\n` : code blocks.\n\n\nYour practice\nIn 2014, the City of Cambridge passed a local ordinance on building energy use disclosure. Spend a few moments reviewing this website to become familiar with the ordinance (in general). Then, use text formatting to add 3-5 sentences as a Markdown section below your YAML that explain‚Äîin your own words‚Äîthe following:\n\nWhat Building Energy Use Disclosure Ordinance is about?\nWhat does the ordinance require?\nWhy do advocates argue that building energy use disclosure is desirable?\n\nYou may edit your text either in the ‚ÄúSource‚Äù or ‚ÄúVisual‚Äù panel, or switch them back and forth to get familiar with both. Make sure to make gratuitous use of bold, italics, etc. in your text. You have access to the ‚ÄúMarkdown Quick Reference‚Äù directly from RStudio (Help - Markdown Quick Reference).\nNow you should Save. RStudio will guide you to save the file in the folder where your ‚ÄúRproj‚Äù file is. Then click the ‚ÄúRender‚Äù button on top of your toolbar (or click Ctrl+Shift+K) to render the current document. The document is rendered by the knitr package. You can immediately see your nicely formatted document in a web browser.\n\n\n\nInsert a video\nYou‚Äôve probably already seen a video on the Cambridge webpage. Let‚Äôs assume that the video hosted on that webpage might be useful for others who view our rendered document. We can insert the video in Quarto and it will render in the output when we knit the document. All you need to do is open the embed code for the video itself (i.e., right-click the video - Copy the embed code), then copy-and-paste it into your Markdown section in the ‚ÄúSource‚Äù panel.\n\nWhat we have above is HTML code. A quick explanation without going into lots of detail:\n\nThe &lt;iframe&gt; tag is what inserts the video (or another document) in line with the Markdown text.\nThe src attribute provides the URL for the video\nThe remaining attributes outline the specifics of how the video will appear.\n\nFor our purposes, the most important of these attributes is data-external=‚Äú1‚Äù. This attribute is typically NOT SPECIFIED in the embed code you find on the web by default and YOU WILL NEED TO ADD IT in order for video content to render in your output. This attribute is currently required for the knitr package to properly render video content.\n\nYour practice\nEmbed this video in your Markdown section after the introductory sentences you have written. Knit your notebook again and inspect your work.\n\n\n\nInsert Code Chunks\nAs you know, the other component of a Quarto Document is code chunks. Recall that code chunks are where the R code lives in a notebook. These are easy to spot as shaded blocks leading by {r} as shown below. (In the Source panel, they always have three backticks followed by {r}).\n\n1 + 1\n\n[1] 2\n\n\nIn accordance with the ordinance, the City maintains BEUDO data for individual properties from 2015-2021. You can view the data on the¬†Cambridge Open Data Portal.¬†Take a few moments to explore the dataset by scrolling down the page and viewing the ‚ÄúColumns in this Dataset‚Äù and ‚ÄúTable Preview‚Äù sections in particular. Now download this dataset in CSV format and save it in your ‚Äúdata‚Äù folder, which is already created within your project folder.\nInsert a code chunk in your Quarto document by going to Code - Insert Chunk. I usually use the shortcut key RStudio provides (Alt+Ctrl+I). Type a few things in the chunk and make it look like the following:\n{r label=\"loadtidyverse\", message=FALSE, warning=FALSE}\nlibrary(tidyverse)\nThe label option is not required, but can be helpful when you render the notebook (otherwise you will see ‚Äúunnamed-chunk‚Äù in the Render tab next to the Console tab). The other options (described in greater detail here) suppress all warnings and messages that might otherwise appear in the rendered HTML output, making it unnecessarily cluttered and hard to read.\nRun this code chunk to load tidyverse package. You can either click the green triangle on the top-right of this chunk, or use Ctrl+Enter.\n\nYour practice\nIn your Console, type in ?readr::read_csv and hit Enter. Review the documentation for read_csv paying special attention to the ‚ÄúArguments‚Äù and ‚ÄúExamples‚Äù sections of the help page. Note that the double colon above tells us that the function read_csv is located within the readr package, which happens to be part of the tidyverse suite of R packages.\n\nInsert a new code chunk and write a line of code that imports your data stored in the .csv file and assigns it to an object called energy.\nUse the colnames() function to get a quick sense of the column names of this dataset.\n\nRun your code chunks. You will be able to see in the rendered output the dimensions of the dataset (always written as rows x columns), the names of the columns, and the data type of each column.\n\n\n\nExamine a subset of your data\nWe can also explore some of the tidyverse package functionality described in the second assigned reading for today. Take another look at this, then proceed to insert a new code chunk like the one below:\n\nmit_energy &lt;- energy |&gt; \n  filter(Owner == \"MASSACHUSETTS INSTITUTE OF TECHNOLOGY\") |&gt; \n  filter(!is.na(`Total GHG Emissions (Metric Tons CO2e)`))\n\nThe above code uses the pipe |&gt; operator to ‚Äúchain together‚Äù lines of code. You can type it in using Shift+Ctrl+M.\nOur code above uses the filter function from the dplyr package (which is also part of tidyverse suite) to create a new dataset that only contains energy use records from MIT buildings that are not missing the total GHG emission attribute.\nNow we are going to create some summary data using the MIT_energy dataset. Go ahead and create another code chunk like the one below:\n\nmit_energy |&gt; \n  group_by(`Data Year`) |&gt; \n  summarise(count = n())\n\n# A tibble: 7 √ó 2\n  `Data Year` count\n        &lt;dbl&gt; &lt;int&gt;\n1        2015   129\n2        2016   134\n3        2017   130\n4        2018   134\n5        2019   111\n6        2020   115\n7        2021   138\n\n\nWe use dplyr::group_by¬†function applied to it such that observations (i.e., rows) are grouped according to the value of this attribute, then the result is passed to the¬†dplyr::summarise¬†function to generate a total number of records per year. By default, the¬†n()¬†function generates a new attribute (i.e., column), which we here name as ‚Äúcount‚Äù. ¬†\nBecause there is no¬†&lt;-¬†operator, the resulting table is displayed but¬†it is not stored¬†in an object that we can go back to later.\nWe can use the same group_by + summarise chain to calculate the total GHG emissions per year, just by identifying the right column to summarise. Note that we are now giving a new name to the year column too.\n\nmit_energy |&gt; \n  group_by(year = `Data Year`) |&gt; \n  summarise(count = n(),\n            total_emission = sum(`Total GHG Emissions (Metric Tons CO2e)`))\n\n# A tibble: 7 √ó 3\n   year count total_emission\n  &lt;dbl&gt; &lt;int&gt;          &lt;dbl&gt;\n1  2015   129        204461.\n2  2016   134        193679.\n3  2017   130        198031.\n4  2018   134        197218.\n5  2019   111        168946.\n6  2020   115        160568.\n7  2021   138        195230.\n\n\n\nYour practice\n\nModify the last code chunk to add more summary results for all MIT buildings. In a new code chunk, please display a table that shows the following results:\n\n\nthe total number of reports per year,\nthe total GHG emission per year,\nthe average GHG emissions (divide your total_emission by count),\nand the average GHG emission intensity (use the column Total GHG Emissions Intensity (kgCO2e/ft2)).\n\nYour table should look like something like this:\n\n\n\n\n\n\nWe mentioned that our resulting table is only displayed but not stored, now in your code chunk, add summary_data &lt;- to store your output as a new object. Run the code and you should now see this object in RStudio Environment panel.\n\nCompare your results with the following code. If you used the same variable name as our example, you should be able to directly run the subsequent code chunk and plot the emission stats over the years! (We will talk more about plotting so don‚Äôt worry if it now looks unfamiliar).\n\nsummary_data &lt;- mit_energy |&gt; \n    group_by(year = `Data Year`) |&gt; \n    summarise(count = n(),\n              total_emission = mean(`Total GHG Emissions (Metric Tons CO2e)`),\n              avg_emission = total_emission/count,\n              avg_intensity = mean(`Total GHG Emissions Intensity (kgCO2e/ft2)`)) \n\n\nplot1 &lt;- ggplot(summary_data)+\n  geom_line(aes(x = year, y = avg_emission))\n  \nplot2 &lt;- ggplot(summary_data)+\n  geom_line(aes(x = year, y = avg_intensity))\n\nrequire(gridExtra)\ngrid.arrange(plot1, plot2, nrow = 2)\n\n\n\n\nDoes this graph tell you something? To complete your working document, please write a few sentences describing your insights through analyzing this dataset. Now Save, Render it again, and Celebrate!\n------\nIn this lab we have introduced how to create and develop a Quarto Document. We have also introduced some commonly-used dplyr funtions including filter, group_by and summarise. This is a soft introduction to dplyr and data wrangling in advance of the main event in Week 2."
  },
  {
    "objectID": "labs/lab2.html",
    "href": "labs/lab2.html",
    "title": "Exploratory Data Analysis with ",
    "section": "",
    "text": "Overview\nThis week‚Äôs Lab Exercise is designed to help you apply some of the tools available within the tidyverse for data wrangling. This particular exercise focuses on the dplyr package and the ggplot2 package. It also begins to engage with data visualization best practices by demonstrating how to create and interpret a variety of graphics with the ggplot2 package.\nExploratory data analysis (EDA) is a phase of a larger data science workflow‚Äîor perhaps a philosophy‚Äîthat emphasizes getting to know the data before rushing to analyze it using this more rigid approaches like hypothesis tests. EDA relies heavily on the creation and interpretation of graphics in order to build familiarity and gain fundamental insights that can inform more sophisticated analyses later on. Data visualization is arguably the most important tool for exploratory data analysis because the information conveyed by graphical display can be very quickly absorbed and because it is generally easy to recognize patterns in a graphical display. There are several overarching goals of exploratory data analysis, including:\n\nTo determine if there are any problems with your dataset.\nTo determine whether the question you are asking can be answered by the data that you have.\nTo begin formulating an answer to your question.\n\n\n\nOur study topic today\nIn the 2017 Tax Cuts and Jobs Act, a new federal incentive was introduced to encourage investment in low-income and undercapitalized communities. States were given the chance to select specific census tracts as Opportunity Zones, where investors could enjoy tax benefits for their eligible investments. Although, there‚Äôs been a lot of curiosity among practitioners and researchers regarding how effective the program is and whether the designations made by governors were successful.\nIf you are interested in the locations of these Opportunity Zones, you can check out this map. The pink geometries reflected on the map are census tracts, which we often use as a proxy for neighborhoods, especially in urban areas. Just find a familiar place, and see which areas have been designated as Opportunity Zones.\nNow please navigate to Urban Institute‚Äôs website about Opportunity Zones, find the link ‚ÄúDownload tract-level data on all Opportunity Zones‚Äù, and download this dataset to your ‚Äúdata‚Äù folder within your Lab 2 project folder. Open it in Excel and take a quick look. This data lists tracts nationwide that were designated an Opportunity Zones, along with essential Census demographic data that describe these tracts.\nWhen done well, exploratory data analysis is guided by the question(s) you are trying to answer. The central question we are exploring in this Lab Exercise is: ‚ÄúIs there a distinguishable difference in economic conditions between designated and not-designated census tracts?‚Äù We will explore our data to first determine if there are problems with the dataset then to decide if you can answer your question(s) with this dataset.\nTo stay organized, you should load packages at the beginning of your script or markdown document.\n\n\nRead and examine our data\nUse read_xlsx from the readxl package will read Microsoft Excel files into data tables.\nNow look at the ‚ÄúEnvironment‚Äù panel on the top-right of your R interface, you should see the new variable ozs. You can also see this variable 27 variables (columns) and 42178 observations (rows). Click it to preview the content of ozs. Alternatively, you could preview it by typing View(ozs) in your console.\nHere are the column definitions:\n\ngeoid: combined state, county, tract FIPS (Federal Information Processing Standards) code this is a unique identification number for each census tract. If it is the first time you heard of tracts, they are sub-areas of a county defined for the purpose of taking a census.\nstate: the name of the state\ncounty: the county name\nDesignated: 1 if the tract was designated an Opportunity Zone\nType: category for OZ designation\nPopulation: total population\nPovertyRate: poverty rate\nmedhhincome: median household income\nmedrent: median gross rent (per month)\nvacancyrate: residential vacancy rate\nunemprate: unemployment rate\npctwhitealone: white population (%)\npctblackalone: black population (%)\npctHispanic: latino population (%)\nMetro: tract in a metropolitan area\n\n\n\nYour practice\nThere are commonly used commands in base R that provide an initial check of a dataset, for example:\n\ndim(), ncol(), nrow()\ncolnames()\nglimpse()\nhead(), tail()\nstr()\nsummary()\n\nInsert a new code chunk and experiment with a few of these functions. What each function accomplishes?\n\nBeside the basic data structures, There are a few other things I‚Äôll encourage you to inspect during the initial check process:\nCompleteness\nBy viewing the first several rows of the dataset, we can already spot some N/A values. For example, there is a field named Designated which is 1 when an eligible tract was designated as an opportunity zone, and N/A where the tract was not designated. (In fact, for our analysis, it‚Äôs better to recode these NAs to equal 0 instead, which we will do in a bit).\nThere are N/As in many of the statistics fields as well, which indicate unavailable information for that specific data point. (If you never see ‚ÄúN/A‚Äù in the first 20 some rows, it will be too soon).\nHow many missing values are there and would that be a hurdle for my analysis? It will be great to have a sense of completeness in terms of what proportion of a field actually holds data. DataExplorer is a handy tool to quickly understand datasets.\n\nDataExplorer::plot_missing(ozs)\n\n\n\n\nUnique values of categorical variables\nThe unique() base R function extracts unique elements in a large set of values. We can use it to specific columns to see what entries we have here. It also helps us to see if there is anything we need to clean up, such as typos or incorrect names, before proceeding to more analysis. For this dataset, we can take a look at the state column. What we have here in this column are all U.S. territories as well as a few NA values.\n\nunique(ozs$state) \n\n [1] \"Alabama\"                  \"Alaska\"                  \n [3] \"Arizona\"                  \"Arkansas\"                \n [5] \"California\"               \"Colorado\"                \n [7] \"Connecticut\"              \"Delaware\"                \n [9] \"District of Columbia\"     \"Florida\"                 \n[11] \"Georgia\"                  \"Hawaii\"                  \n[13] \"Idaho\"                    \"Illinois\"                \n[15] \"Indiana\"                  \"Iowa\"                    \n[17] \"Kansas\"                   \"Kentucky\"                \n[19] \"Louisiana\"                \"Maine\"                   \n[21] \"Maryland\"                 \"Massachusetts\"           \n[23] \"Michigan\"                 \"Minnesota\"               \n[25] \"Mississippi\"              \"Missouri\"                \n[27] \"Montana\"                  \"Nebraska\"                \n[29] \"Nevada\"                   \"New Hampshire\"           \n[31] \"New Jersey\"               \"New Mexico\"              \n[33] \"New York\"                 \"North Carolina\"          \n[35] \"North Dakota\"             \"Ohio\"                    \n[37] \"Oklahoma\"                 \"Oregon\"                  \n[39] \"Pennsylvania\"             \"Rhode Island\"            \n[41] \"South Carolina\"           \"South Dakota\"            \n[43] \"Tennessee\"                \"Texas\"                   \n[45] \"Utah\"                     \"Vermont\"                 \n[47] \"Virginia\"                 \"Washington\"              \n[49] \"West Virginia\"            \"Wisconsin\"               \n[51] \"Wyoming\"                  \"American Samoa\"          \n[53] NA                         \"Guam\"                    \n[55] \"Northern Mariana Islands\" \"Puerto Rico\"             \n[57] \"Virgin Islands\"          \n\n\nRange of numerical variables\nFor numeric columns, it‚Äôll be helpful to visually inspect whether the values fall within the expected range, how the values are distributed, and whether there are any wacky numbers like -999 (often used in SPSS files to indicate missing data), etc. A quick histogram helps us to see the value distribution. Additionally, it visualizes patterns by dividing the data set into groups (or bins) of equal length, then communicating how many or what proportion of the observations fall within each of those ‚Äúbins‚Äù.\n\n# We can use the base R function hist() to check one variable:\nhist(ozs$Population)\n\n\n\n\n\n# Or use the DataExplorer package to check multiple variables at the same time\nDataExplorer::plot_histogram(ozs[,c(10:13)])\n\n\n\n\n\n\n\nYour practice\nCheck the DesignatedOZ column - what values does it contain?\n\n\n\n\nData Cleanning\nThe Urban Institute has coded the designated variable as either taking a value of 1 when designated or NA when not. We can recode the NA values in DesignatedOZ for legibility. In the following code, we uses the dplyr function: mutate to modify DesignatedOZ in place. We replaced the numbers with texts since the NA and 1 here have no mathematical meaning.\n\nozs &lt;- ozs |&gt; mutate(DesignatedOZ = \n                ifelse(is.na(DesignatedOZ), \"Not Designated\", \"Designated\"))\n\nThere are a few columns (such as SE_Flag) that we won‚Äôt need for this analysis. We can use select in dplyr function to select a subset of columns to work on. select allows you to retain specified columns. If there is a minus sign in front, that means to drop these specified columns.\n\nozs &lt;- ozs |&gt; select(-c(dec_score, SE_Flag, Metro, Micro, NoCBSAType))\n\n\n\nExploring Variation Within Variables\nThe code chunk below creates a boxplot to contrast the distribution of poverty rates between designated opportunity zones and undesignated zones. A boxplot is a very commonly used EDA tool that allows us to quickly visualize the distribution of a single variable or column of data if we are working with a data frame. Note that we are using what should now be familiar conventions to construct the graphic beginning with the ggplot function, then adding more features with the + operator and other functions listed in the package reference.\n\nggplot(data = ozs): This is the main plotting function. ozs is your dataset we use.\ngeom_boxplot(): Recall that geometric layers are called geoms. It tells R what kind of geometry you want to use visualize the data.\naes(x = DesignatedOZ, y = PovertyRate): The aes() function is where you tell ggplot which variable goes on the x axis followed by which variable goes on the y axis.\nThe labs function sets the labels. Because the legend is showing the fill component of the plot, we use the fill argument in the labs function to set the name of the legend itself.\nWe used a new function scale_y_continuous to specify y axis properties. Here we are making sure the poverty rate are labeled as percentages. If you remove this line, they will by default show as decimal numbers.\n\n\nggplot(data = ozs) +\n  geom_boxplot(aes(x = DesignatedOZ, y = PovertyRate, fill = DesignatedOZ)) + \n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"\", x = \"Opportunity Zone Eligible Tracts\", y = \"Poverty Rate\", fill = \"Tracts\")\n\n\n\n\nWe can create cross-tabulations or contingency tables as a complement to visual EDA tools like boxplots and histograms. An easy way to do this is to use existing functions in the dplyr package. A brief demonstration is given in the code chunk below.\n\nozs |&gt; \n  count(state, DesignatedOZ)\n\n# A tibble: 108 √ó 3\n   state          DesignatedOZ       n\n   &lt;chr&gt;          &lt;chr&gt;          &lt;int&gt;\n 1 Alabama        Designated       158\n 2 Alabama        Not Designated   677\n 3 Alaska         Designated        25\n 4 Alaska         Not Designated    43\n 5 American Samoa Designated        16\n 6 Arizona        Designated       168\n 7 Arizona        Not Designated   702\n 8 Arkansas       Designated        85\n 9 Arkansas       Not Designated   435\n10 California     Designated       879\n# ‚Ñπ 98 more rows\n\n\n\nExercise 1\nInsert a new code chunk and a markdown section below this one to ‚Äúcatch‚Äù your responses. Please review what we have learned and proceed with the questions below.\n\nWhich of the variables (columns) are continuous and which are categorical (e.g., factor)?\n\nHint: Recall that a variable is categorical if it can only take one of a small set of values and continuous if it can take any of an infinite set of ordered values.\nWhich function or approach did you use to answer this question?\n\nAre there variables (columns) with missing data values?\n\nWhich function or approach did you use to answer this question?\n\nCreate a graphic that contrasts the distribution of the unemployment rate in designated zones and in undesignated zones in this dataset.\n\nInterpret the graphic(s) you have created and include 2-3 sentences of text explanation (i.e., in an RMarkdown section)\n\n\n\n\n\nExploring Variation Between Variables\nWe are often interested in bivariate relationships or how two variables relate to one another. Scatterplots are often used to visualize the association between two continuous variables. They can reveal much about the nature of the relationship between two variables.\nLet‚Äôs use a subset of our data - tracts in Massachusetts - to perform this part of analysis. You can definitely use the entire dataset, it‚Äôs just there will be over 40,000 points showing on the graph.\n\nozs_ma &lt;- ozs |&gt; filter(state == \"Massachusetts\") \n\nNow we begin by creating a scatterplot of poverty rate and unemployment rate. A theme template is applied here for a cleaner look.\n\nggplot(ozs_ma) +\n  geom_point(aes(unemprate, PovertyRate)) +\n  labs(title = \"Poverty rate vs. unemployment rate in Opportunity Zone eligible tracts\", \n       subtitle = \"State of Massachusetts\",\n       x = \"Unemployment rate\",\n       y = \"Poverty rate\",\n       caption = \"Source: Urban Institute (2018)\") + \n  theme_bw()\n\n\n\n\nWhat can we say about the hypothesized relationship between the unemployment rate and the poverty rate? As we move from left to right along the x-axis (i.e., as unthe employment rate creases), the amount of poverty rate reported also increases.\nAs a complement to the scatterplot, we can use the base R cor function to calculate the (Pearson by default, see the documentation for other options) correlation between any continuous variables in the dataset. The DataExplorer package is also designed to help us quickly understand patterns in our data. We demonstrate both in the following code.\nIf you are unfamiliar with reading a correlation matrix, the values range between -1 and 1 where:\n\n-1 indicates a perfectly negative linear correlation between two variables\n0 indicates no linear correlation between two variables\n1 indicates a perfectly positive linear correlation between two variables\n\n\nozs_ma |&gt; select(Population:medrent, pctwhite:BAorhigher) |&gt; \n  na.omit() |&gt; \n  stats::cor(use = \"complete.obs\")\n\n                Population medhhincome PovertyRate  unemprate    medvalue\nPopulation    1.0000000000   0.1971310 -0.17219731 -0.1255690  0.05060663\nmedhhincome   0.1971310072   1.0000000 -0.74280330 -0.5823090  0.45672297\nPovertyRate  -0.1721973144  -0.7428033  1.00000000  0.5848120 -0.15673933\nunemprate    -0.1255690086  -0.5823090  0.58481204  1.0000000 -0.32627434\nmedvalue      0.0506066324   0.4567230 -0.15673933 -0.3262743  1.00000000\nmedrent       0.1434824928   0.6494977 -0.32058250 -0.4139894  0.60946764\npctwhite      0.0007742062   0.4334837 -0.58166274 -0.4583927  0.01479243\npctBlack      0.0305972149  -0.2081070  0.24067220  0.3002354  0.07662521\npctHispanic  -0.0610330071  -0.4470198  0.53817440  0.4107687 -0.24108925\npctAAPIalone  0.1179507879   0.1360442  0.05777953 -0.1516819  0.38403023\npctunder18    0.0075944281  -0.3886837  0.28931873  0.4263952 -0.48597184\npctover64    -0.0200934534   0.1116477 -0.40837729 -0.2236285 -0.08790793\nHSorlower    -0.0510755304  -0.6192728  0.38428550  0.5080771 -0.59138685\nBAorhigher    0.0317046620   0.5837037 -0.24483958 -0.4850706  0.71630822\n                 medrent      pctwhite    pctBlack pctHispanic pctAAPIalone\nPopulation    0.14348249  0.0007742062  0.03059721 -0.06103301   0.11795079\nmedhhincome   0.64949767  0.4334836763 -0.20810703 -0.44701977   0.13604425\nPovertyRate  -0.32058250 -0.5816627414  0.24067220  0.53817440   0.05777953\nunemprate    -0.41398938 -0.4583927470  0.30023542  0.41076874  -0.15168191\nmedvalue      0.60946764  0.0147924343  0.07662521 -0.24108925   0.38403023\nmedrent       1.00000000  0.0592656294 -0.01942961 -0.21839927   0.37309846\npctwhite      0.05926563  1.0000000000 -0.64391155 -0.72603098  -0.14608318\npctBlack     -0.01942961 -0.6439115534  1.00000000  0.05756055  -0.07150764\npctHispanic  -0.21839927 -0.7260309801  0.05756055  1.00000000  -0.16352352\npctAAPIalone  0.37309846 -0.1460831806 -0.07150764 -0.16352352   1.00000000\npctunder18   -0.47107334 -0.4866220916  0.28987714  0.52756318  -0.29072583\npctover64    -0.20439363  0.5440272257 -0.22702697 -0.42430798  -0.23991183\nHSorlower    -0.59290214 -0.4609030805  0.16871183  0.56358387  -0.25180077\nBAorhigher    0.67357448  0.3278345358 -0.19790460 -0.42390213   0.38422347\n               pctunder18    pctover64   HSorlower   BAorhigher\nPopulation    0.007594428 -0.020093453 -0.05107553  0.031704662\nmedhhincome  -0.388683691  0.111647722 -0.61927277  0.583703714\nPovertyRate   0.289318732 -0.408377288  0.38428550 -0.244839584\nunemprate     0.426395199 -0.223628488  0.50807715 -0.485070585\nmedvalue     -0.485971837 -0.087907931 -0.59138685  0.716308223\nmedrent      -0.471073339 -0.204393629 -0.59290214  0.673574479\npctwhite     -0.486622092  0.544027226 -0.46090308  0.327834536\npctBlack      0.289877141 -0.227026967  0.16871183 -0.197904602\npctHispanic   0.527563183 -0.424307981  0.56358387 -0.423902133\npctAAPIalone -0.290725828 -0.239911831 -0.25180077  0.384223470\npctunder18    1.000000000 -0.248306018  0.68045309 -0.718007353\npctover64    -0.248306018  1.000000000 -0.14789121 -0.003268322\nHSorlower     0.680453091 -0.147891209  1.00000000 -0.923184800\nBAorhigher   -0.718007353 -0.003268322 -0.92318480  1.000000000\n\n\n\nozs_ma |&gt; select(Population:medrent, pctwhite:BAorhigher) |&gt; \n  na.omit() |&gt; \n  DataExplorer::plot_correlation()\n\n\n\n\nAn additional note to the code above is that we selected several continuous variables that we want to inspect, and removed NA values so that the cocorrelationalues can be correctly calculated.\n\nWhat can we do if we are interested in statistical associations between categorical variables? The typical approach is to summarise the values under each category and visualize them using barcharts.\nIn the following code, you can see our familiar group_by + summarise process used to calculate the average median house income by county in Massachusetts. This summarized table is then piped to ggplot() for visualization.\n\nozs_ma |&gt; \n  group_by(county, DesignatedOZ) |&gt;  \n  summarise(\n    Tracts = n(),\n    Income = mean(medhhincome, na.rm=TRUE)) |&gt; \n  ggplot() +\n  geom_col(aes(x = county, y = Income, fill = DesignatedOZ), \n           position = \"dodge\") \n\n\n\n\n\nExercise 2\nTake a few minutes to read this bar chart below:\n\nHow can we modify our code to replicate the bar chart in this image? You‚Äôll notice that you can achieve most of the features by tweaking our previous examples, plus a little bit more exploration. In a new code chunk, please copy and paste our last bar chart code, and try your best to address the following questions.\n\nPlease add the title, subtitle, x- and y-axis labels, and the data source annotation to your bar chart.\nThe background looks much cleaner. Please choose a theme template for your bar chart.\nThe x-axis labels are titled to 45 degrees. How can I achieve this? Hint.\nThe labels on the y-axis are formatted in thousands with commas. This can be achieved by using the same function scale_y_continuous(labels = scales::percent) we have seen before. Hint.\nLastly, the counties are not arranged alphabetically, but rather by the income values mapped to the y-axis, starting from large to small. How can I achieve this? Hint.\n\n\n\n\nWork Products\nPlease submit a .qmd file and knitted HTML file that shows your work and responses for each of the Exercises included in this lab. Look back at our EDA question ‚ÄúIs there a distinguishable difference in economic conditions between designated and not-designated census tracts?‚Äù Discuss what you have found so far and how you would like to further investigate this question.\nAlso, briefly comment on your experience with R during this lab exercise. Please upload your report to Canvas by the end of day, Tuesday, Nov 7."
  },
  {
    "objectID": "resources/analysis.html",
    "href": "resources/analysis.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Urban analytics draws upon statistics, visualization, and computation to better understand and ultimately shape cities. This course emphasizes telling stories about cities and neighborhoods covering a set of fundamental concepts of descriptive approaches, quantitative and spatial analysis in R, and principles of reproducible data analysis. Students learn to communicate the results of visualization and analysis for use in decision-making and policy development and to critique those processes."
  },
  {
    "objectID": "resources/analysis.html#this-website-is-under-construction-please-check-back-later-for-updates",
    "href": "resources/analysis.html#this-website-is-under-construction-please-check-back-later-for-updates",
    "title": "Applied Data Science for Cities",
    "section": "This website is under construction, please check back later for updates! üòÑ",
    "text": "This website is under construction, please check back later for updates! üòÑ"
  },
  {
    "objectID": "resources/data.html",
    "href": "resources/data.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Urban analytics draws upon statistics, visualization, and computation to better understand and ultimately shape cities. This course emphasizes telling stories about cities and neighborhoods covering a set of fundamental concepts of descriptive approaches, quantitative and spatial analysis in R, and principles of reproducible data analysis. Students learn to communicate the results of visualization and analysis for use in decision-making and policy development and to critique those processes."
  },
  {
    "objectID": "resources/data.html#this-website-is-under-construction-please-check-back-later-for-updates",
    "href": "resources/data.html#this-website-is-under-construction-please-check-back-later-for-updates",
    "title": "Applied Data Science for Cities",
    "section": "This website is under construction, please check back later for updates! üòÑ",
    "text": "This website is under construction, please check back later for updates! üòÑ"
  },
  {
    "objectID": "resources/index.html",
    "href": "resources/index.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Urban analytics draws upon statistics, visualization, and computation to better understand and ultimately shape cities. This course emphasizes telling stories about cities and neighborhoods covering a set of fundamental concepts of descriptive approaches, quantitative and spatial analysis in R, and principles of reproducible data analysis. Students learn to communicate the results of visualization and analysis for use in decision-making and policy development and to critique those processes."
  },
  {
    "objectID": "resources/index.html#this-website-is-under-construction-please-check-back-later-for-updates",
    "href": "resources/index.html#this-website-is-under-construction-please-check-back-later-for-updates",
    "title": "Applied Data Science for Cities",
    "section": "This website is under construction, please check back later for updates! üòÑ",
    "text": "This website is under construction, please check back later for updates! üòÑ"
  },
  {
    "objectID": "resources/inspiration.html",
    "href": "resources/inspiration.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Urban analytics draws upon statistics, visualization, and computation to better understand and ultimately shape cities. This course emphasizes telling stories about cities and neighborhoods covering a set of fundamental concepts of descriptive approaches, quantitative and spatial analysis in R, and principles of reproducible data analysis. Students learn to communicate the results of visualization and analysis for use in decision-making and policy development and to critique those processes."
  },
  {
    "objectID": "resources/inspiration.html#this-website-is-under-construction-please-check-back-later-for-updates",
    "href": "resources/inspiration.html#this-website-is-under-construction-please-check-back-later-for-updates",
    "title": "Applied Data Science for Cities",
    "section": "This website is under construction, please check back later for updates! üòÑ",
    "text": "This website is under construction, please check back later for updates! üòÑ"
  },
  {
    "objectID": "resources/textbook.html",
    "href": "resources/textbook.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Urban analytics draws upon statistics, visualization, and computation to better understand and ultimately shape cities. This course emphasizes telling stories about cities and neighborhoods covering a set of fundamental concepts of descriptive approaches, quantitative and spatial analysis in R, and principles of reproducible data analysis. Students learn to communicate the results of visualization and analysis for use in decision-making and policy development and to critique those processes."
  },
  {
    "objectID": "resources/textbook.html#this-website-is-under-construction-please-check-back-later-for-updates",
    "href": "resources/textbook.html#this-website-is-under-construction-please-check-back-later-for-updates",
    "title": "Applied Data Science for Cities",
    "section": "This website is under construction, please check back later for updates! üòÑ",
    "text": "This website is under construction, please check back later for updates! üòÑ"
  },
  {
    "objectID": "syllabus/index.html",
    "href": "syllabus/index.html",
    "title": "Applied Data Science for Cities",
    "section": "",
    "text": "Description\nUrban data science draws upon statistics, visualization, and computation to better understand and ultimately shape cities. This course emphasizes telling stories about cities using a set of descriptive approaches, quantitative and spatial analysis in R. We will learn how to describe community characteristics with small area census data, work with local administrative data, and think about how our analysis of quantitative data fit with other forms of data and engagement to fill in gaps in knowledge.\nLearning objectives:\n\nGain familiarity with R as a data analysis, mapping, and storytelling tool.\nBe able to identify relevant open datasets in support of your research question.\nArticulate the main arguments for your data analysis product and how it supports planning and decision-making regarding an issue of local/national significance.\n\n\n\nHow Will We Be Learning\nLecture: Class meetings are generally divided into lecture (Mondays) and laboratory sessions (Wednesdays) that focus on concepts and hand-on applications, respectively.\nLab: We will provide data science tutorials using R. Each lab tutorial aims to solve a specific urban data science problem in addition to building coding skills. Lab reports are due before the subsequent lab period and should be written independently.\nExtension readings: We provide a light material list that focuses on specific topics each week. These resources are meant to expand your knowledge and enhance your project completion capabilities.\nUrban data science project: The term project for the course will focus on integrating the tools of data science to explore a specific real-world planning issue or research question. This is a group project and students will define the scope of the project and identify specific deliverable(s) early in the semester. Reproducing an existing analysis or study using different data or an alternate study area is also acceptable for the term project.\n\n\nPrerequisites\nThis is a relatively fast-paced course so students can benefit from some prior knowledge working in R and RStudio. However, this is not a course prerequisite. Our first course sessions will focus on ensuring that we are all familiar with some of the basic work environment and methods which we‚Äôll make use of over the semester.\n\n\nAssessment\n\n\n\n\n\n\n\nAssignment\nWeight\n\n\n\n\nLab Reports\nPackage Introduction\nProject Scope Memo\nProject Presentation\nAttendance\n40%\n10%\n10%\n30%\n10%\n\n\n\n\n\nKey Logistics\nLab Exercises: You will be working on lab on most Wednesday classes and submit a short report before the subsequent class. Details will be specified in each of the assignments distributed at the beginning of the lab session.\nPackage Introduction: Each student is expected to introduce one package to the class that is relevant to your research interests. The objectives are (1) to foster a collaborative approach to acquiring familiarity with the ever-increasing new packages, and (2) to identify valuable existing R packages applicable to your study. A separate guideline will be distributed at the beginning of the course.\nProject Scope Memo: Due Friday, Nov 17.\nThe purpose of this memo is to communicate the scope of your project (what you will do) and your strategies for collecting, visualizing, and analyzing data (how you will do it). A separate guideline will be distributed at the beginning of the course.\nProject Presentation: Week 8 (Dec 11 ‚Äì Dec 13), class time.\n\n\nLate policy\nTo keep all students on a relatively level playing field, a late assignment will be accepted up until one week after the original due date for a loss of a half-letter grade (e.g., an A becomes an A-). After one week, late assignments will receive no credit and will not be accepted.\n\n\nSoftware\nWe use R and R Studio as the coding environment to develop analysis and applications. You can install the software on your personal computers from here and here.\n\n\nCommunication\nPlan on using our class Slack channel, email, and office hours to get help with troubleshooting problems as they arise in your work. I also encourage you to work with others in the class to troubleshoot problems - it is highly likely that others in the class have encountered similar problems, and this also allows us to create a repository of our problems and responses.\nEmail: I check emails quite frequently, but I will not always be able to respond to emails right away. Please plan accordingly so that you don‚Äôt miss deadlines.\nSlack: We have a Slack workspace that is accessible through Canvas for general communication, including homework Q&A, resource exchange, project collaboration, etc.\nOffice hours: Please consult the top of the syllabus for specific times. I will announce if there are any changes or exceptions. I‚Äôm happy to answer any specific coding questions, or chat and help shape the objective and scope of your projects.\n\n\nEthics\nAcademic Integrity:¬†Violations to academic integrity are unacceptable at MIT and DUSP.¬†Instances of misconduct include but are not limited to plagiarism, cheating, deliberately unauthorized use of course data and material.\nCollaboration Policy:¬†While team collaboration is encouraged, students should specify their role and tasks in a project. A positive and constructive attitude for teamwork is essential for a successful completion of the course.\nDiversity and Inclusion:¬†MIT highly values a diverse, friendly, respectful, and inclusive learning environment among students, faculty, and staff. We welcome all individuals regardless of their origin, citizenship, gender identity, sexual orientation, religious and pollical beliefs. Please contact me or departmental staff if you have any question / consideration regarding this."
  }
]